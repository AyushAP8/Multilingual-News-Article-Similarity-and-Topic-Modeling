{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.initializers import GlorotUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, units: int):\n",
    "        \"\"\" The encoder model for the src sentences.\n",
    "            It contains an embedding part and a GRU part.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: The src vocabulary size\n",
    "            embedding_size: The embedding size for the embedding layer\n",
    "            units: Number of hidden units in the RNN (GRU) layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here\n",
    "        # Note: Please know what the decoder needs from encoder. This determines the parameters of the GRU layer\n",
    "        self.emb = Embedding(vocab_size, embedding_size, embeddings_initializer=GlorotUniform(seed=0), name='Encoder Embedding')\n",
    "        self.lstm = LSTM(units, recurrent_initializer=GlorotUniform(seed=0), return_state=True, return_sequences=True, name='Encoder LSTM')\n",
    "        #self.den = Dense(vocab_size, kernel_initializer=GlorotUniform(seed=0), Dropout=Dropout(rate=0.1, seed=0))\n",
    "        # End\n",
    "\n",
    "    def call(self, src_ids, src_mask):\n",
    "        \"\"\" Encoder forward\n",
    "        Args:\n",
    "            src_ids: Tensor, (batch_size x max_len), the token ids of input sentences in a batch\n",
    "            src_mask: Tensor, (batch_size x max_len), the mask of the src input. True value in the mask means this timestep is valid, otherwise this timestep is ignored\n",
    "        Returns:\n",
    "            enc_output: Tensor, (batch_size x max_len x units), the output of GRU for all timesteps\n",
    "            final_state: Tensor, (batch_size x units), the state of the final valid timestep\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1. Retrieve embedding\n",
    "        #      2. GRU\n",
    "        # Please refer to the calling arguments of GRU: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#call-arguments\n",
    "        emb = self.emb(src_ids)\n",
    "        enc_outputs, final_state = self.lstm(emb, mask=src_mask)\n",
    "        #x = self.den(last_state)\n",
    "        # End\n",
    "        return enc_outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, units: int, dropout_rate: float):\n",
    "        \"\"\" The decoder model for the tgt sentences.\n",
    "            It contains an embedding part, a GRU part, a dropout part, and a classifier part.\n",
    "            \n",
    "        Args:\n",
    "            vocab_size: The tgt vocabulary size\n",
    "            embedding_size: The embedding size for the embedding layer\n",
    "            units: Number of hidden units in the RNN (GRU) layer\n",
    "            dropout_rate: The classifier has a (units x vocab_size) weight. This is a large weight matrix. We apply a dropout layer to avoid overfitting.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Start your code here\n",
    "        # Note: 1. Please correctly set the parameter of GRU\n",
    "        #       2. No softmax here because we will need the sequence to sequence loss later\n",
    "        self.emb = Embedding(vocab_size, embedding_size, embeddings_initializer=GlorotUniform(seed=0), name='Decoder Embedding')\n",
    "        self.lstm = LSTM(units, kernel_initializer=GlorotUniform(seed=0), return_state=True, return_sequences=True, name='Encoder GRU')\n",
    "        self.den = Dense(vocab_size, activation='relu', kernel_initializer=GlorotUniform(seed=0), name='Decoder Dense')\n",
    "        self.dropout = Dropout(rate=dropout_rate, seed=0)\n",
    "        # End\n",
    "\n",
    "    def call(self, tgt_ids, initial_state, tgt_mask):\n",
    "        \"\"\" Decoder forward.\n",
    "            It is called by decoder(tgt_ids=..., initial_state=..., tgt_mask=...)\n",
    "\n",
    "        Args:\n",
    "            tgt_ids: Tensor, (batch_size x max_len), the token ids of input sentences in a batch\n",
    "            initial_state: Tensor, (batch_size x units), the state of the final valid timestep from the encoder\n",
    "            tgt_mask: Tensor, (batch_size x max_len), the mask of the tgt input. True value in the mask means this timestep is valid, otherwise this timestep is ignored\n",
    "        Return:\n",
    "            dec_outputs: Tensor, (batch_size x max_len x vocab_size), the output of GRU for all timesteps\n",
    "        \"\"\"\n",
    "        # Start your code here\n",
    "        # Step 1. Retrieve embedding\n",
    "        #      2. GRU\n",
    "        #      3. Apply dropout to the GRU output\n",
    "        #      4. Classifier\n",
    "        # Note: Please refer to the calling arguments of GRU: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU#call-arguments\n",
    "        x = self.emb(tgt_ids)\n",
    "        gru_outputs, _ = self.lstm(x, mask=tgt_mask, initial_state=initial_state)\n",
    "        gru_outputs = self.dropout(gru_outputs)\n",
    "        dec_outputs = self.den(gru_outputs)\n",
    "        # End\n",
    "        return dec_outputs\n",
    "    \n",
    "    def predict(self, tgt_ids, initial_state):\n",
    "        \"\"\" Decoder prediction.\n",
    "            This is a step in recursive prediction. We use the previous prediction and state to predict current token.\n",
    "            Note that we only need to use the gru_cell instead of GRU becasue we only need to calculate one timestep.\n",
    "            \n",
    "        Args:\n",
    "            tgt_ids: Tensor, (batch_size, ) -> (1, ), the token id of the current timestep in the current sentence.\n",
    "            initial_state: Tensor, (batch_size x units) -> (1 x units), the state of the final valid timestep from the encoder or the previous hidden state in prediction.\n",
    "        Return:\n",
    "            dec_outputs: Tensor, (batch_size x vocab_size) -> (1 x vocab_size), the output of GRU for this timestep.\n",
    "            state: Tensor, (batch_size x units) -> (1 x units), the state of this timestep.\n",
    "        \"\"\"\n",
    "        lstm_cell = self.lstm.cell\n",
    "        # Start your code here\n",
    "        # Step 1. Retrieve embedding\n",
    "        #      2. GRU Cell, see https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell#call-arguments\n",
    "        #      3. Classifier (No dropout)\n",
    "        x = self.emb(tgt_ids)\n",
    "        gru_outputs, state = lstm_cell(x, states=initial_state, training=False)\n",
    "        #gru_outputs = self.dropout(gru_outputs, training=False)\n",
    "        dec_outputs = self.den(gru_outputs)\n",
    "        # End\n",
    "        return dec_outputs, state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "303fab501bce6d78710df28f707cec632ea5e4a0792c121319b8d58840bfae92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
